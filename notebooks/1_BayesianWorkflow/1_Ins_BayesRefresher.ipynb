{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Bayesian refresher and Introduction to ArviZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What does an end to end Bayesian Workflow look like?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* Refresh our understanding of Bayes Theorom\n",
    "* Fit a small binomial model\n",
    "* Show how a full statistical workflow, even outside of Bayesian Methods, requires more steps more than just model fitting\n",
    "* Introduce ArviZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "### The most common formulation\n",
    "$$\n",
    "\\Large\n",
    "P(\\theta \\mid y) = \\frac{ P(y) \\mid \\theta)p(\\theta)}{p(y)}\n",
    "$$\n",
    "\n",
    "### Breaking it down\n",
    "####  $P(\\theta \\mid y)$ -> Posterior distribution\n",
    "_\"What is the distribution of parameters given the observed data?\"_  \n",
    "\n",
    "After obtaining data, or making observations, what is our belief regarding the parameters of the underlying statistical model? \n",
    "\n",
    "* Estimating the posterior distribution is the goal of Bayesian analysis. \n",
    "* The process of estimating the posterior distribution often refered to as **Inference**\n",
    "* There are numerous ways to perform inference, each with their own pros and cons\n",
    "    * In this tutorial we will only be using Markov Chain Monte Carlo (MCMC)\n",
    "\n",
    "####  $P(y \\mid \\theta )$ -> Likelihood\n",
    "_\"What is the probability of the observed data given a model parameter value\"_  \n",
    "\n",
    "Likelihood functions tell us how \"likely\" the observed data is, for all the possible parameter values. Likelihood and loss functions, from \"Machine Learning\", perform roughly the same role, which is evaluating \"good\" of a set of model parameters are at explaining the data.\n",
    "\n",
    "####  $P(\\theta)$ -> Prior\n",
    "_\"What is the probability of distribution given no observations\"_  \n",
    "Before we've observed any data what is the plausible probability distribution of parameters?\n",
    "\n",
    "####  $P(y)$ -> Marginal Probability of Evidence\n",
    "\n",
    "What is the probability distribution of data? In most cases this term is difficult or impossible to calculate, so much so that most inference techniques cleverly get around their calculations. MCMC is one of those techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative formulations\n",
    "\n",
    "### Likelihood notation\n",
    "I particularly like this formulation because clearly demarcates difference between Likelihood and probability terms  \n",
    "\n",
    "$$ P(\\theta | y) = \\frac{ L(\\theta | y)p(\\theta)}{p(y} $$\n",
    "\n",
    "### Defined as a proportion\n",
    "While the posterior, likelihood, and prior are usually *distributions*, the denomiator is a scalar that normalizes the numerator. In many modern Bayesian Inference Methods\n",
    "\n",
    "$$ P(\\theta | y) \\propto P(y | \\theta)p(\\theta) $$\n",
    "\n",
    "### Defined with puppies\n",
    "Even if you hate math, you'd have to be a monster to hate puppies. This pictorial formula is taken from  John Kruschke's excellent book [Doing Bayesian Data Analysis](https://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0124058884) Do note the lazy puppy on the right. The laziness is an indication of how little work this puppy does in most Bayesian Inference methods.\n",
    "![BayesianPuppies](../../img/Doing-DBA.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
