{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5.1 Model Comparison Methods\n",
    "*How can we determine which model better fits our needs?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Path Constants\n",
    "if os.path.split(os.getcwd())[-1] != \"notebooks\":\n",
    "    os.chdir(os.path.join(\"..\"))\n",
    "\n",
    "from utils import metropolis_hastings\n",
    "\n",
    "NETCDF_DIR = \"inference_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* Understanding of how to interpret WAIC, PSIS-LOO numerical metrics\n",
    "* Understanding of how to interpret plot_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infinite parameters and Infinite Models\n",
    "As Bayesian modelers not only do we have to handle infinite model parameters, we also have to handle infinite model defnitions.\n",
    "\n",
    "Take our water example from Section 1.3. \n",
    "$$\n",
    "\\theta = Uniform(0,1) \\\\\n",
    "p_{water} = Binom(\\theta)\n",
    "$$\n",
    "In this model we're evaulating not just one possible proportion of water on a planet, but an infinite amount of proportions from 0 to 1.\n",
    "\n",
    "But this begs the question why 0 to 1, or why even this model? This is also a valid model\n",
    "\n",
    "$$\n",
    "\\theta = Beta(1,5) \\\\\n",
    "p_{water} = Binom(\\theta)\n",
    "$$\n",
    "\n",
    "as is this model\n",
    "\n",
    "$$ \n",
    "\\theta = Uniform(0,1) \\\\\n",
    "\\sigma = Uniform(0,100) \\\\\n",
    "p_{water} = Norm(\\theta, \\sigma)\n",
    "$$\n",
    "\n",
    "### How do we pick?\n",
    "The flexibility to design whatever model you like is simultaneously the beauty and challenge of Bayesian modeling. While this philosophy is nice, for the practioner there still is the question of which model to choose. The reason this question is particularly challenging in Bayesian Statistics because we don't get just one prediction, we get an infinite amount of predictions in a distribution. However this does not mean that Bayesian analysis is not suspect to phenomena such as overfitting or excess complexity.\n",
    "\n",
    "Luckily there tools that help. In particular we'll cover *Widely Applicable Information Criterion* and how it's used in cojunction with `plot_compare`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A history of Information Theory\n",
    "When you hear Information Criterion you usually hear the words entropy, divergence, and deviance.  Let's briefly cover how these ideas build upon each other by walking through information theory.\n",
    "\n",
    "\n",
    "\\*Note:* This section heavily draws from Richard McElreath's examples. We'll be covering the material at a high level to get a full understanding we highly recommend his  [lectures](https://www.youtube.com/watch?v=gjrsYDJbRh0) and [books](https://www.amazon.com/Statistical-Rethinking-Bayesian-Examples-Chapman/dp/1482253445/). \n",
    "\n",
    "\n",
    "To restate our prior question \"If have  3, or 5, or 10, estimates of the truth (also called models) which most usefully explains the truth, given the truth is also uncertain?\"\n",
    "\n",
    "Over the years a number of folks have contributed mathematical tools that help answer this question. In short the we go in this order\n",
    "\n",
    "1. How do we quantify uncertainty? (Information Entropy)\n",
    "2. How do we quantify the additional uncertainty added from an estimate when we know the truth? (KL Divergence)\n",
    "3. How do we estimate the additional uncertainty added from an estimate when we don't know the truth? (Deviance)\n",
    "4. How do we make sure we're ensure were underestimating uncertainty with complex models? (Information Criteria)\n",
    "\n",
    "This section is quite heavy of math. However it is not critical to understand the fine details, but more so the path of thinking.\n",
    "\n",
    "#### *Information Entropy*: Quantifying Uncertainty\n",
    "Weather is uncertain, and dice rolls are uncertain. But weather in certain areas is delightfully consistent, like my hometown of Orange County, and \"loaded\" are designed to be inconsistent.\n",
    "\n",
    "Information Entropy is a summarization of our uncertainty through the following formula  \n",
    "$$ H(p) = -\\sum_{n=1}^{N} p_{i}log(p_{i})$$\n",
    "\n",
    "#### *Divergence*: Quantifying additional uncertainty added from estimations of the truth\n",
    "Now that we can quantify the uncertainty in the distribution of reality, we can compare the uncertainty in the distribution our model to see how far off we are. The formula is as follows, where $p$ is the truth and $q$ is the estimate\n",
    "\n",
    "$$D_{kl} = -\\sum_{n=1}^{N} p_{i}(log(q_{i}) - log(p_{i}))$$\n",
    "\n",
    "This is the idea of **divergence**, a measurement of the \"distance\" between two distributions. One important note this distance is not symmetric. For some quick intuition It was way more surprising for me to go from always sunny Los Angeles and experience snow in Wisconsin, than it is for a native Wisconsite to see sun in Los Angeles. People in Los Angeles only see sun, people in Wisconsin see sun and snow, therefore a Los Angeles is much farther removed from the reality of a mixed sunny/snowy climate, than a Wisconsinite is to an always sunny climate.\n",
    "\n",
    "#### *Deviance*: Estimating additional uncertainty because we don't know the truth using Deviance\n",
    "If we knew the truth then there wouldn't be a point to this tutorial, or the entire field of Statistics/Machine Learning. Since we don't know the truth this unfortunately means we can't use the formula above, but through some clever thinking it turns out you don't need to know the truth to compare two models, just how far off one model is relative from another. By using the truth data (The weather in the past), and a model estimates, its possible to calculate **Deviance** which is an estimate of the divergence.\n",
    "\n",
    "Taken from Osvaldo Martin's book [Bayesian Data Analysis in Python](https://www.packtpub.com/big-data-and-business-intelligence/bayesian-analysis-python-second-edition)\n",
    "\n",
    "$$D_{kl}(p \\mid \\mid q) - D_{kl}(q \\mid \\mid r) \\approx \\sum_{n=1}^{N} log(q_{i}) - \\sum_{n=1}^{N} log(r_{i})$$\n",
    "Remember this works because we're not trying to see how far one model is from the truth, we're just trying to see how far two models are from the truth *relative to each other*\n",
    "\n",
    "#### *Information Criterion*: Making sure we're not overfitting to observed data using complex models\n",
    "Deviance uses data that has been observed to make an estimation. As it turns out it's impossible to use data you haven't seen because you haven't seen it. As it also turns out if you make models complex you can explain anything perfectly. *Information Criterion* uses both estimates of deviance and a penalizing for excess model complexity to \"score\" a model fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Criterion in detail\n",
    "Information Criterion calcuations are composed of two terms\n",
    "\n",
    "1. Estimate of a the deviance\n",
    "2. Number of parameters in the model\n",
    "\n",
    "Below is the summarized formula for Widely Applicable Information Criterion (Also Watanabeâ€“Akaike information criterion) \n",
    "\n",
    "$$WAIC = -2 lppd + p_{WAIC}$$ \n",
    "\n",
    "*lppd* stands for log pointwise predictive density, and $p_{WAIC}$ is an effective number of the parameters. Between the two we get an estimate of model \"accuracy\" balanced by model complexity\n",
    "\n",
    "### What about AIC, BIC, DIC?\n",
    "There are numerous variations of Information Criterion (IC) estimates. However due to assumptions in the calculations some of the IC's estimations are less general than others. As the name suggests Widely Applicable IC, is well, the most widely applicable. Richard McElreath goes through the full explanation in his book.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare\n",
    "Compare Dataframe is a handy way of comparing a set of models using Information Criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>waic</th>\n",
       "      <th>p_waic</th>\n",
       "      <th>d_waic</th>\n",
       "      <th>weight</th>\n",
       "      <th>se</th>\n",
       "      <th>dse</th>\n",
       "      <th>warning</th>\n",
       "      <th>waic_scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>non centered</th>\n",
       "      <td>61.3022</td>\n",
       "      <td>0.820067</td>\n",
       "      <td>0</td>\n",
       "      <td>0.515575</td>\n",
       "      <td>2.54078</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>deviance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>centered</th>\n",
       "      <td>61.4296</td>\n",
       "      <td>0.919548</td>\n",
       "      <td>0.127437</td>\n",
       "      <td>0.484425</td>\n",
       "      <td>2.50275</td>\n",
       "      <td>0.106882</td>\n",
       "      <td>0</td>\n",
       "      <td>deviance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 waic    p_waic    d_waic    weight       se       dse  \\\n",
       "non centered  61.3022  0.820067         0  0.515575  2.54078         0   \n",
       "centered      61.4296  0.919548  0.127437  0.484425  2.50275  0.106882   \n",
       "\n",
       "             warning waic_scale  \n",
       "non centered       0   deviance  \n",
       "centered           0   deviance  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = az.load_arviz_data(\"non_centered_eight\")\n",
    "data2 = az.load_arviz_data(\"centered_eight\")\n",
    "compare_dict = {\"non centered\": data1, \"centered\": data2}\n",
    "az.compare(compare_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>waic</th>\n",
       "      <th>p_waic</th>\n",
       "      <th>d_waic</th>\n",
       "      <th>weight</th>\n",
       "      <th>se</th>\n",
       "      <th>dse</th>\n",
       "      <th>warning</th>\n",
       "      <th>waic_scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>non centered</th>\n",
       "      <td>-30.6511</td>\n",
       "      <td>0.820067</td>\n",
       "      <td>0</td>\n",
       "      <td>0.515568</td>\n",
       "      <td>1.25817</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>centered</th>\n",
       "      <td>-30.7148</td>\n",
       "      <td>0.919548</td>\n",
       "      <td>0.0637183</td>\n",
       "      <td>0.484432</td>\n",
       "      <td>1.23771</td>\n",
       "      <td>0.0534409</td>\n",
       "      <td>0</td>\n",
       "      <td>log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 waic    p_waic     d_waic    weight       se        dse  \\\n",
       "non centered -30.6511  0.820067          0  0.515568  1.25817          0   \n",
       "centered     -30.7148  0.919548  0.0637183  0.484432  1.23771  0.0534409   \n",
       "\n",
       "             warning waic_scale  \n",
       "non centered       0        log  \n",
       "centered           0        log  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.compare(compare_dict, scale=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load two versions \n",
    "In order\n",
    "1. WAIC is the WAIC \n",
    "2. p_waic is the estimate of number of parameters\n",
    "3. Weight is the Akaike weight which can be used for model average\n",
    "4. se is the standard error of the WAIC estimate (Remember WAIC estimates are also distributions)\n",
    "5. dse is the standard error of the top ranked WAIC and each model\n",
    "6. Warning that WAIC may be failing\n",
    "7. Scale is the \"multiplier\" on the in sample deviation portion of WAIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Compare\n",
    "ArviZ provides plot compare as a way to summarize the results of WAIC, and in particular when comparing multiple models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c1a1eef28>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_compare = az.compare({'Centered 8 schools': az.load_arviz_data('centered_eight'),\n",
    "                           'Non-centered 8 schools': az.load_arviz_data('non_centered_eight')})\n",
    "az.plot_compare(model_compare, plot_ic_diff=True, plot_standard_error=True, insample_dev=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The open circles are the WAIC values of each model. This includes both the in sample deviance and the model complexity penalization.\n",
    "* The dark circles are in sample deviance. These measure how well the model fits the data.\n",
    "* The standard error of the differences ofplotted in by the gray triangles and gray lines. This is the standard error of the different \n",
    "* The dark gray lines are the standard error of WAIC\n",
    "\n",
    "All except WAIC can be turned on or off using the boolean argument shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enough with the math, what's the practical advice\n",
    "* Models with lower Information Criterion tend to be better\n",
    "* Lowest Information Criterion does not always mean the best model\n",
    "\n",
    "That being said the interpretation of Information Criterion is not always straightforward. Interpretation depends on your data, how your models choice and parametrization, priors etc. Remember it's just a tool to help you understand your models, and not a strict rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Methods Pareto Smoothed Importance Sample Leave one out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave One Out validation is another method to estimate a model's ability to fit unseen data.\n",
    "\n",
    "Leave One Out validation is a method where the model is trained on all the data points but one, then the posterior is used to estimate the likelihood of the last point. This method is an effective way of testing a models ability to fit unseeen data, but unfortunately requires the Inference run to be repeated N times for N data points, this is usually takes too long.\n",
    "\n",
    "PSIS-Loo, developed by Aki Vehtari, does some magic to pareto sample stuff and make this easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
